{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5844058b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMonte Carlo Tree Search (MCTS) algorithm implementation and testing using an openAI gym enviroment. \\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Monte Carlo Tree Search (MCTS) algorithm implementation and testing using an openAI gym enviroment. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5711803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the ALE/SpaceInvaders-v5 environment there are: 6 possible actions.\n",
      "In the ALE/SpaceInvaders-v5 environment the observation is composed of: 210 values.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We will test our Monte Carlo Tree Search algorithm (MCTS) using an openAI gym environment named \"CartPole\".\n",
    "You can read more information about the environment at this link:\n",
    "https://www.gymlibrary.ml/environments/classic_control/cart_pole/\n",
    "\n",
    "Feel free to change the environment with other as you like, changing the game name variable, \n",
    "but keep in mind that for this MCTS implementation both Actions and Observation must be Discrete. \n",
    "'''\n",
    "\n",
    "# uncomment these lines below if you get a runtime error of gym package not found\n",
    "# change the path value using your actual gym path using the 'pip show gym' command\n",
    "#import sys\n",
    "#path = \"c:\\\\users\\\\my\\\\anaconda3\\\\envs\\\\mcts\\\\lib\\\\site-packages\"\n",
    "#sys.path.append(path)\n",
    "\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "from utils import moving_average\n",
    "from stack_frame import preprocess_frame, stack_frame\n",
    "\n",
    "GAME_NAME = 'ALE/SpaceInvaders-v5'\n",
    "\n",
    "env = gym.make(GAME_NAME)\n",
    "    \n",
    "GAME_ACTIONS = env.action_space.n\n",
    "GAME_OBS = env.observation_space.shape[0]\n",
    "\n",
    "print('In the ' + GAME_NAME + ' environment there are: ' + str(GAME_ACTIONS) + ' possible actions.')\n",
    "print('In the ' + GAME_NAME + ' environment the observation is composed of: ' + str(GAME_OBS) + ' values.')\n",
    "\n",
    "env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e39b1f",
   "metadata": {},
   "source": [
    "View the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb4b6227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of frame is:  (210, 160, 3)\n",
      "No. of Actions:  6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The size of frame is: \", env.observation_space.shape)\n",
    "print(\"No. of Actions: \", env.action_space.n)\n",
    "env.reset()\n",
    "plt.figure()\n",
    "#plt.imshow(env.reset())\n",
    "#plt.title('Original Frame')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af949a1b",
   "metadata": {},
   "source": [
    "Preprocess the frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "853033f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.figure()\n",
    "env_reset, _ = env.reset()\n",
    "plt.imshow(preprocess_frame(env_reset, (8, -12, -12, 4), 84), cmap=\"gray\")\n",
    "plt.title('Pre Processed image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b20244",
   "metadata": {},
   "source": [
    "Stack the frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "634709a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(frames, state, is_new=False):\n",
    "    frame = preprocess_frame(state, (8, -12, -12, 4), 84)\n",
    "    frames = stack_frame(frames, frame, is_new)\n",
    "\n",
    "    return frames\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09d603b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from copy import deepcopy\n",
    "from math import *\n",
    "import random\n",
    "\n",
    "c = 1.0\n",
    "\n",
    "class Node:\n",
    "    \n",
    "    '''\n",
    "    The Node class represents a node of the MCTS tree. \n",
    "    It contains the information needed for the algorithm to run its search.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, game, done, parent, observation, action_index):\n",
    "          \n",
    "        # child nodes\n",
    "        self.child = None\n",
    "        \n",
    "        # total rewards from MCTS exploration\n",
    "        self.T = 0\n",
    "        \n",
    "        # visit count\n",
    "        self.N = 0        \n",
    "                \n",
    "        # the environment\n",
    "        self.game = game\n",
    "        \n",
    "        # observation of the environment\n",
    "        self.observation = observation\n",
    "        \n",
    "        # if game is won/loss/draw\n",
    "        self.done = done\n",
    "\n",
    "        # link to parent node\n",
    "        self.parent = parent\n",
    "        \n",
    "        # action index that leads to this node\n",
    "        self.action_index = action_index\n",
    "        \n",
    "        \n",
    "    def getUCBscore(self):\n",
    "        \n",
    "        '''\n",
    "        This is the formula that gives a value to the node.\n",
    "        The MCTS will pick the nodes with the highest value.        \n",
    "        '''\n",
    "        \n",
    "        # Unexplored nodes have maximum values so we favour exploration\n",
    "        if self.N == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        # We need the parent node of the current node \n",
    "        top_node = self\n",
    "        if top_node.parent:\n",
    "            top_node = top_node.parent\n",
    "            \n",
    "        # We use one of the possible MCTS formula for calculating the node value\n",
    "        return (self.T / self.N) + c * sqrt(log(top_node.N) / self.N)\n",
    "    \n",
    "    \n",
    "    def detach_parent(self):\n",
    "        # free memory detaching nodes\n",
    "        del self.parent\n",
    "        self.parent = None\n",
    "       \n",
    "        \n",
    "    def create_child(self):\n",
    "        \n",
    "        '''\n",
    "        We create one children for each possible action of the game, \n",
    "        then we apply such action to a copy of the current node enviroment \n",
    "        and create such child node with proper information returned from the action executed\n",
    "        '''\n",
    "        \n",
    "        if self.done:\n",
    "            return\n",
    "    \n",
    "        actions = []\n",
    "        games = []\n",
    "        for i in range(GAME_ACTIONS): \n",
    "            actions.append(i)           \n",
    "            new_game = deepcopy(self.game)\n",
    "            games.append(new_game)\n",
    "            \n",
    "        child = {} \n",
    "        for action, game in zip(actions, games):\n",
    "            observation, reward, done, _, _ = game.step(action)\n",
    "            child[action] = Node(game, done, self, observation, action)                        \n",
    "            \n",
    "        self.child = child\n",
    "                \n",
    "            \n",
    "    def explore(self):\n",
    "        \n",
    "        '''\n",
    "        The search along the tree is as follows:\n",
    "        - from the current node, recursively pick the children which maximizes the value according to the MCTS formula\n",
    "        - when a leaf is reached:\n",
    "            - if it has never been explored before, do a rollout and update its current value\n",
    "            - otherwise, expand the node creating its children, pick one child at random, do a rollout and update its value\n",
    "        - backpropagate the updated statistics up the tree until the root: update both value and visit counts\n",
    "        '''\n",
    "        \n",
    "        # find a leaf node by choosing nodes with max U.\n",
    "        \n",
    "        current = self\n",
    "        \n",
    "        while current.child:\n",
    "\n",
    "            child = current.child\n",
    "            max_U = max(c.getUCBscore() for c in child.values())\n",
    "            actions = [ a for a,c in child.items() if c.getUCBscore() == max_U ]\n",
    "            if len(actions) == 0:\n",
    "                print(\"error zero length \", max_U)                      \n",
    "            action = random.choice(actions)\n",
    "            current = child[action]\n",
    "            \n",
    "        # play a random game, or expand if needed          \n",
    "            \n",
    "        if current.N < 1:\n",
    "            current.T = current.T + current.rollout()\n",
    "        else:\n",
    "            current.create_child()\n",
    "            if current.child:\n",
    "                current = random.choice(current.child)\n",
    "            current.T = current.T + current.rollout()\n",
    "            \n",
    "        current.N += 1      \n",
    "                \n",
    "        # update statistics and backpropagate\n",
    "            \n",
    "        parent = current\n",
    "            \n",
    "        while parent.parent:\n",
    "            \n",
    "            parent = parent.parent\n",
    "            parent.N += 1\n",
    "            parent.T = parent.T + current.T           \n",
    "            \n",
    "            \n",
    "    # def rollout(self):\n",
    "        \n",
    "    #     '''\n",
    "    #     The rollout is a random play from a copy of the environment of the current node using random moves.\n",
    "    #     This will give us a value for the current node.\n",
    "    #     Taken alone, this value is quite random, but, the more rollouts we will do for such node,\n",
    "    #     the more accurate the average of the value for such node will be. This is at the core of the MCTS algorithm.\n",
    "    #     '''\n",
    "        \n",
    "    #     if self.done:\n",
    "    #         return 0        \n",
    "        \n",
    "    #     v = 0\n",
    "    #     done = False\n",
    "    #     new_game = deepcopy(self.game)\n",
    "    #     while not done:\n",
    "    #         action = new_game.action_space.sample()\n",
    "    #         new_action = new_game.step(action)\n",
    "    #         # print(\"new_action\", new_action)\n",
    "            \n",
    "    #         observation, reward, done, _, _ = new_game.step(action)\n",
    "    #         v = v + reward\n",
    "    #         if done:\n",
    "    #             new_game.reset()\n",
    "    #             new_game.close()\n",
    "    #             break             \n",
    "    #     return v\n",
    "\n",
    "    def rollout(self):\n",
    "        '''\n",
    "        The rollout is a play from a copy of the environment of the current node using the DQN model.\n",
    "        This will give us a value for the current node.\n",
    "        Taken alone, this value is quite random, but, the more rollouts we will do for such node,\n",
    "        the more accurate the average of the value for such node will be. This is at the core of the MCTS algorithm.\n",
    "        '''\n",
    "        policy_net = torch.load('policy_net_weights_SPACEDQN.pth')\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if self.done:\n",
    "            return 0        \n",
    "\n",
    "        v = 0\n",
    "        done = False\n",
    "        new_game = deepcopy(self.game)\n",
    "        while not done:\n",
    "            # Use DQN model to select action\n",
    "            state = torch.from_numpy(self.observation).float().unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(state).max(1)[1].view(1, 1)\n",
    "            _, reward, done, _ = new_game.step(action.item())\n",
    "            v = v + reward\n",
    "            if done:\n",
    "                new_game.reset()\n",
    "                new_game.close()\n",
    "                break             \n",
    "        return v\n",
    "\n",
    "    \n",
    "    def next(self):\n",
    "        \n",
    "        ''' \n",
    "        Once we have done enough search in the tree, the values contained in it should be statistically accurate.\n",
    "        We will at some point then ask for the next action to play from the current node, and this is what this function does.\n",
    "        There may be different ways on how to choose such action, in this implementation the strategy is as follows:\n",
    "        - pick at random one of the node which has the maximum visit count, as this means that it will have a good value anyway.\n",
    "        '''\n",
    "\n",
    "        if self.done:\n",
    "            raise ValueError(\"game has ended\")\n",
    "\n",
    "        if not self.child:\n",
    "            raise ValueError('no children found and game hasn\\'t ended')\n",
    "        \n",
    "        child = self.child\n",
    "        \n",
    "        max_N = max(node.N for node in child.values())\n",
    "       \n",
    "        max_children = [ c for a,c in child.items() if c.N == max_N ]\n",
    "        \n",
    "        if len(max_children) == 0:\n",
    "            print(\"error zero length \", max_N) \n",
    "            \n",
    "        max_child = random.choice(max_children)\n",
    "        \n",
    "        return max_child, max_child.action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c861643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "MCTS_POLICY_EXPLORE = 30 # MCTS exploring constant: the higher, the more reliable, but slower in execution time\n",
    "\n",
    "# def Policy_Player_MCTS(mytree):  \n",
    "\n",
    "#         '''\n",
    "#         Our strategy for using the MCTS is quite simple:\n",
    "#         - in order to pick the best move from the current node:\n",
    "#             - explore the tree starting from that node for a certain number of iterations to collect reliable statistics\n",
    "#             - pick the node that, according to MCTS, is the best possible next action\n",
    "#         '''\n",
    "        \n",
    "#         for i in range(MCTS_POLICY_EXPLORE):\n",
    "#             mytree.explore()\n",
    "            \n",
    "#         _, next_action = mytree.next()\n",
    "            \n",
    "#         # note that here we are detaching the current node and returning the sub-tree \n",
    "#         # that starts from the node rooted at the choosen action.\n",
    "#         # The next search, hence, will not start from scratch but will already have collected information and statistics\n",
    "#         # about the nodes, so we can reuse such statistics to make the search even more reliable!\n",
    "#         mytree.detach_parent()\n",
    "        \n",
    "#         return next_action\n",
    "\n",
    "def Policy_Player_MCTS(mytree):  \n",
    "\n",
    "    '''\n",
    "    Our strategy for using the MCTS is quite simple:\n",
    "    - in order to pick the best move from the current node:\n",
    "        - explore the tree starting from that node for a certain number of iterations to collect reliable statistics\n",
    "        - pick the node that, according to MCTS, is the best possible next action\n",
    "    '''\n",
    "    \n",
    "    for i in range(MCTS_POLICY_EXPLORE):\n",
    "        mytree.explore()\n",
    "        \n",
    "    next_tree, next_action = mytree.next()\n",
    "        \n",
    "    # note that here we are detaching the current node and returning the sub-tree \n",
    "    # that starts from the node rooted at the choosen action.\n",
    "    # The next search, hence, will not start from scratch but will already have collected information and statistics\n",
    "    # about the nodes, so we can reuse such statistics to make the search even more reliable!\n",
    "    next_tree.detach_parent()\n",
    "    \n",
    "    return next_tree, next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4c93563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode #1\n",
      "starting while\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got tuple)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting while\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     new_tree, action \u001b[38;5;241m=\u001b[39m \u001b[43mPolicy_Player_MCTS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmytree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minloop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m     mytree \u001b[38;5;241m=\u001b[39m new_tree\n",
      "Cell \u001b[0;32mIn[56], line 38\u001b[0m, in \u001b[0;36mPolicy_Player_MCTS\u001b[0;34m(mytree)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mOur strategy for using the MCTS is quite simple:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m- in order to pick the best move from the current node:\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    - explore the tree starting from that node for a certain number of iterations to collect reliable statistics\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    - pick the node that, according to MCTS, is the best possible next action\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MCTS_POLICY_EXPLORE):\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mmytree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m next_tree, next_action \u001b[38;5;241m=\u001b[39m mytree\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# note that here we are detaching the current node and returning the sub-tree \u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# that starts from the node rooted at the choosen action.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# The next search, hence, will not start from scratch but will already have collected information and statistics\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# about the nodes, so we can reuse such statistics to make the search even more reliable!\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 124\u001b[0m, in \u001b[0;36mNode.explore\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# play a random game, or expand if needed          \u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current\u001b[38;5;241m.\u001b[39mN \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 124\u001b[0m     current\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m=\u001b[39m current\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m \u001b[43mcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     current\u001b[38;5;241m.\u001b[39mcreate_child()\n",
      "Cell \u001b[0;32mIn[55], line 190\u001b[0m, in \u001b[0;36mNode.rollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m new_game \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# Use DQN model to select action\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    192\u001b[0m         action \u001b[38;5;241m=\u001b[39m policy_net(state)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got tuple)"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "episodes = 10\n",
    "rewards = []\n",
    "moving_average = []\n",
    "# game = gym.make(GAME_NAME, render_mode='human')\n",
    "game = gym.make(GAME_NAME)\n",
    "'''\n",
    "Here we are experimenting with our implementation:\n",
    "- we play a certain number of episodes of the game\n",
    "- for deciding each move to play at each step, we will apply our MCTS algorithm\n",
    "- we will collect and plot the rewards to check if the MCTS is actually working.\n",
    "- For CartPole-v0, in particular, 200 is the maximum possible reward. \n",
    "'''\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    reward_e = 0    \n",
    "    observation = game.reset() \n",
    "    # done = False\n",
    "    \n",
    "    new_game = deepcopy(game)\n",
    "    mytree = Node(new_game, False, 0, observation, 0)\n",
    "    \n",
    "    print('episode #' + str(e+1))\n",
    "    \n",
    "    while True:\n",
    "        print(\"starting while\")\n",
    "        new_tree, action = Policy_Player_MCTS(mytree)\n",
    "        print(\"inloop\")\n",
    "\n",
    "        mytree = new_tree\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = game.step(action)  \n",
    "        done = terminated or truncated\n",
    "                        \n",
    "        reward_e = reward_e + reward\n",
    "        \n",
    "        # game.render() # uncomment this if you want to see your agent in action!\n",
    "                \n",
    "        if done:\n",
    "            print('reward_e ' + str(reward_e))\n",
    "            # game.close()\n",
    "            break\n",
    "        \n",
    "    rewards.append(reward_e)\n",
    "    moving_average.append(np.mean(rewards[-100:]))\n",
    "    \n",
    "plt.plot(rewards)\n",
    "plt.plot(moving_average)\n",
    "plt.show()\n",
    "print('moving average: ' + str(np.mean(rewards[-20:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
